{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8409e87-c7a1-427d-83b1-7b96c6bfba40",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1bdfe-a8a7-401a-814a-f5a0a572c54a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, \\\n",
    "StringType, IntegerType, TimestampType, DateType, FloatType\n",
    "import time\n",
    "import pathlib\n",
    "from pyspark.sql.functions import col, to_date, sum, avg, max, min, \\\n",
    "stddev, percentile_approx,\\\n",
    "pandas_udf, PandasUDFType, lit, udf, collect_list, sqrt, monotonically_increasing_id, map_from_entries,\\\n",
    "rank, dense_rank, count, when\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4f330-522a-41ad-b2e2-e3012d603121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAll([\\\n",
    "            ('spark.app.name', 'Glucose_Analysis_Spark')])\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .getOrCreate()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78224625-863f-4ac1-a89f-bf9d5f296ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "  StructField('PatientId', StringType(), True),\n",
    "  StructField('Value', FloatType(), True),\n",
    "  StructField('GlucoseDisplayDate', DateType(), True)\n",
    "  ])\n",
    "\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "df = spark.createDataFrame(emptyRDD,schema)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7fa6eb-5a1a-4943-b706-a8a8e1aa149e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet('/cephfs/stepped_glucose_data/step0_load/parquet_0_to_10/part-00000-532ee45d-8e0d-44c4-8f3b-884b22175e0f-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f93d3-cc66-4759-bf85-69b70d8cd854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('y_binary', lit(1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "799e3b76-b1fc-43f5-b627-1c06ed0a6b68",
   "metadata": {
    "tags": []
   },
   "source": [
    "import numpy as np\n",
    "    \n",
    "# def calculate_poincare(values):\n",
    "#     glucose_differentials = np\n",
    "#.diff(values.collect())\n",
    "\n",
    "#     st_dev_differentials = np.std(np.diff(glucose_differentials))\n",
    "#     st_dev_values = np.std(glucose_differentials)\n",
    "\n",
    "#     # measures the width of poincare cloud\n",
    "#     short_term_variation = (1 / np.sqrt(2)) * st_dev_differentials\n",
    "\n",
    "#     # measures the length of the poincare cloud\n",
    "#     long_term_variation = np.sqrt((2 * st_dev_values ** 2) - (0.5 * st_dev_differentials ** 2))\n",
    "#     return round(short_term_variation, 3), \\\n",
    "#            round(long_term_variation, 3), \\\n",
    "#            round(short_term_variation / long_term_variation, 3)\n",
    "\n",
    "def calculate_poincare(values):\n",
    "    glucose_differentials = values\n",
    "\n",
    "    st_dev_differentials = stddev(glucose_differentials.diff())\n",
    "    st_dev_values = stddev(glucose_differentials)\n",
    "\n",
    "    # measures the width of poincare cloud\n",
    "    short_term_variation = (1 / sqrt(2)) * st_dev_differentials\n",
    "\n",
    "    # measures the length of the poincare cloud\n",
    "    long_term_variation = sqrt((2 * st_dev_values ** 2) - (0.5 * st_dev_differentials ** 2))\n",
    "    return round(short_term_variation, 3), \\\n",
    "           round(long_term_variation, 3), \\\n",
    "           round(short_term_variation / long_term_variation, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba92081-2c36-4480-8b27-584a71e3b6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def pyspark_summary_statistics(self, df, spark):\n",
    "# @pandas_udf(StructType([StructField('Entropy', FloatType())]), PandasUDFType.GROUPED_MAP)\n",
    "# def entropy_grouped(df):\n",
    "#     return feat_create_obj.entropy_extraction(df.Value)\n",
    "\n",
    "def entropy_udf(vals):\n",
    "    feat_create_obj = feat_create()\n",
    "    return udf(feat_create_obj.entropy_extraction(vals), FloatType())\n",
    "\n",
    "def poincare_udf(vals):\n",
    "    feat_create_obj = feat_create()\n",
    "    return udf(calculate_poincare(vals),\\\n",
    "               StructType([\\\n",
    "                   StructField('First', FloatType()),\\\n",
    "                   StructField('Second', FloatType()),\\\n",
    "                   StructField('Third', FloatType())\\\n",
    "               ]))\n",
    "\n",
    "# def chunk_by_index():\n",
    "#     return udf(collect_list\n",
    "\n",
    "def create_partition_date(df, chunk_val):\n",
    "    window = Window.partitionBy(df['PatientId']).orderBy(df['GlucoseDisplayTime'])\n",
    "    df = df.select('*', rank().over(window).alias('index'))\n",
    "    df = df.withColumn(\"Chunk\", (df.index/chunk_val).cast(IntegerType()))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def pyspark_summary_statistics(df, \\\n",
    "                               daily_stats_features_lower,\\\n",
    "                               daily_stats_features_upper, \\\n",
    "                               chunk_val = 12):  \n",
    "\n",
    "    df_added = create_partition_date(df, chunk_val)\n",
    "    \n",
    "    group_cols = [\"PatientId\", \"Chunk\"]\n",
    "\n",
    "    summary_df = df_added.groupby(group_cols)\\\n",
    "        .agg(max('y_binary').alias('y_summary_binary'),\\\n",
    "             avg(\"Value\").alias(\"Mean\"),\\\n",
    "             stddev(\"Value\").alias(\"Std Dev\"),\\\n",
    "             percentile_approx(\"Value\", .5).alias(\"Median\"), \\\n",
    "             min(\"Value\").alias(\"Min\"),\\\n",
    "             max(\"Value\").alias(\"Max\"),\\\n",
    "             count(when(col(\"Value\") < daily_stats_features_lower, 1)).alias(\"CountBelow\"),\\\n",
    "             count(when(col(\"Value\") > daily_stats_features_upper, 1)).alias(\"CountAbove\"),\\\n",
    "             (count(when(col(\"Value\") < daily_stats_features_lower, 1))/chunk_val).alias(\"PercentageBelow\"),\\\n",
    "             (count(when(col(\"Value\") > daily_stats_features_upper, 1))/chunk_val).alias(\"PercentageAbove\")\n",
    "            )\n",
    "\n",
    "    df_added = df_added.join(summary_df, ['PatientId', 'Chunk'])\n",
    "    \n",
    "    return df_added"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7976f6a7-b88b-4d21-9823-75f7e21c3d64",
   "metadata": {
    "tags": []
   },
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847f755-6765-4f01-a54e-63d5f5f7caee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyspark_summary_statistics(df, 70, 180, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b14176-2502-4731-8336-ea4c361f480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_daily_features=df.groupby(analysis_group).apply(transform_features)\n",
    "\n",
    "    return added_daily_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a6384-4f88-4020-aa90-cc5a0305fb5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb304db0-ff73-4ec0-b390-ac970bf72bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e07884-c7e8-4865-b26a-04e38350577a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdcbf51-41bc-4198-935d-6dd56bc77055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet('/cephfs/summary_stats/all_val_bool/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec9f7c-597a-48eb-9249-bbbfb48542cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5460fbc-7a79-406a-bd36-0b6faac4a8e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df = df.withColumn('target', when(df.DiffPrevious > 9, 1)\n",
    "                         .when(df.DiffPrevious < -9,-1)\n",
    "                         .otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8986662-706c-41e9-bbe8-7a2c7f1c30be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.select(col('DiffPrevious'), col('target')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7edeb97-64eb-48d3-984b-742ea4d047e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.repartition('NumId').write.parquet('/cephfs/summary_stats/all_val_bool_updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662721d7-7a18-4cbc-a9cf-69b858103171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c179417-1b8d-4cd8-8239-fd36d3c50532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf80ca-ac93-4f76-a63f-943ecab1c022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a76947-dfd6-4d11-a679-4227cbb70170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Glucose\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "training_df_poincare = spark.read.parquet('/cephfs/featuresData/poincare/train')\n",
    "training_df_poincare.show(5)\n",
    "training_df_entropy = spark.read.parquet('/cephfs/featuresData/entropy/train')\n",
    "training_df_entropy.show(5)\n",
    "\n",
    "training_df_complex_features = training_df_poincare.join(training_df_entropy,['NumId', 'Chunk'])\n",
    "training_df_complex_features.show()\n",
    "\n",
    "training_features_summary_stats= spark.read.parquet('/cephfs/summary_stats/encoded/one_hot_train/summary_stats_cohort_bool_encoded.parquet')\n",
    "training_features_summary_stats.show(3)\n",
    "\n",
    "training_df_final = training_df_complex_features.join(training_features_summary_stats,['NumId', 'Chunk'])\n",
    "training_df_final.show(5)\n",
    "\n",
    "training_df_final.repartition('NumId').write.parquet('/cephfs/summary_stats/all_train_bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb6123-f10a-422b-9986-215300379b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
